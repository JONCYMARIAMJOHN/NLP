{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01736496-c0a5-430b-96d4-ae228a9fa50d",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe7c43c-3189-49ce-844c-17c42bb17940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b74c8b-06be-43c7-8591-53195f3a7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c495e32-7787-4443-a8ad-4fba3380b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf298644-c2c8-4641-b3cc-3367382ddd92",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08830d2a-1a45-4ae5-ad6d-53e2c1f4e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Machine_Learning/Study_Material/nlp_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8596b-4871-4427-8849-5dc374b1a547",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af15144-7096-4a57-8817-d5a92be0bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i seriously hate one subject to death but now ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im so full of life i feel appalled</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sit here to write i start to dig out my feel...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ive been really angry with r and i feel like a...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel suspicious if there is no one outside l...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment Emotion\n",
       "0  i seriously hate one subject to death but now ...    fear\n",
       "1                 im so full of life i feel appalled   anger\n",
       "2  i sit here to write i start to dig out my feel...    fear\n",
       "3  ive been really angry with r and i feel like a...     joy\n",
       "4  i feel suspicious if there is no one outside l...    fear"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'] = df['Comment'].str.lower()\n",
    "df['Emotion'] = df['Emotion'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eeefafc-9bcb-499e-9b5d-7821098815b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\joncy\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\joncy\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joncy\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joncy\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joncy\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\joncy\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766dcc0f-8a2a-45f2-bd4c-6028b7a632b4",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4a280-9d5b-4bc1-9e8e-6b34af07f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word tokenize\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in df['Comment'].tolist()]\n",
    "print(tokenized_docs)\n",
    "\n",
    "print(\"\\nSentence tokenization\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_token = [sent_tokenize(doc) for doc in df['Comment'].tolist()]\n",
    "print(sent_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c5bf9-cd43-4786-a087-2aea9b861a53",
   "metadata": {},
   "source": [
    "#### Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f06f5-9cfa-4680-bfc5-c6a0271af4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#This line creates a regular expression pattern to match any character that is included in the string.punctuation constant. \n",
    "#re.escape() is used to escape any special characters within the punctuation string .\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token) # substitute any punctuation characters in the current token (token) with an empty string (''). This effectively removes all punctuation from the token.\n",
    "        if not new_token == u'': # checks if the token after removing punctuation is not an empty string.\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2cd0b-6ff1-4efb-8e74-02b5d55f1e7c",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aed93a-fefe-48e9-a4d4-044981158b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089d78-c248-4a13-b14e-181f556648d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51bbf0-244e-492f-9a8d-92605609880e",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30a4d3-82e5-43e1-9f7a-c8aaa4353f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization\n",
    "from nltk.stem.porter import PorterStemmer # imports the PorterStemmer class from NLTK, which is used for stemming.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #  imports the WordNetLemmatizer class from NLTK, which is used for lemmatization.\n",
    "\n",
    "porter = PorterStemmer() #Creates an instance of the PorterStemmer class\n",
    "wordnet = WordNetLemmatizer() #Creates an instance of the WordNetLemmatizer class\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #final_doc.append(porter.stem(word)) #stems the current word using the Porter Stemmer and appends the stemmed form to the final_doc list.\n",
    "        final_doc.append(wordnet.lemmatize(word)) # lemmatizes the current word using the WordNet Lemmatizer and appends the lemmatized form to the final_doc list.\n",
    "    \n",
    "    preprocessed_docs.append(final_doc) #After processing all words in a document, the resulting list of stemmed or lemmatized words (final_doc) is appended to the preprocessed_docs list.\n",
    "\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62541bc-af46-4103-b585-fc9da61765de",
   "metadata": {},
   "source": [
    "<b>Preprocessing:</b>\n",
    "<p><b>Text Cleaning: Convert the text to lowercase, remove punctuation, and special characters.\n",
    "Tokenization: Split the text into individual words (tokens).\n",
    "Stopword Removal: Remove commonly occurring words (like 'the', 'is', 'in') that donâ€™t add much meaning to the classification task.\n",
    "Impact on Model Performance: Cleaning helps reduce noise in the data, stopword removal focuses the model on more relevant terms, and tokenization allows the model to treat individual words as features.\n",
    "</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bd286-11e9-42e0-8d6c-987b3617b19f",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00db45-0fd2-43cc-81d0-c249bb6fd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Comment'])\n",
    "y = df['Emotion']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec669a-7939-44bf-8e5c-f290ee287181",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850036a0-eff3-4aba-a342-7c06cce04c9c",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fb45f-3ddc-45bf-a2ec-bf81db4a032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb793e76-bce9-4705-b037-e04dfde617a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVC\n",
    "SvcClassifier = SVC(kernel='linear')\n",
    "SvcClassifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "SvcPredictions = SvcClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5392fba-a200-4422-8c92-06e71133f177",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83644a2d-50ec-41d3-9393-7f476590f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Naive Bayes model\n",
    "accuracyNB = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracyNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27226179-8a40-419c-8824-9667921bdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the SVC model\n",
    "accuracySVC = accuracy_score(y_test, SvcPredictions)\n",
    "print(\"Accuracy:\", accuracySVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbbd75c-37c3-4b9e-8381-8217b34845ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.88      0.95      0.91       392\n",
      "        fear       0.92      0.92      0.92       416\n",
      "         joy       0.94      0.87      0.91       380\n",
      "\n",
      "    accuracy                           0.91      1188\n",
      "   macro avg       0.91      0.91      0.91      1188\n",
      "weighted avg       0.91      0.91      0.91      1188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report Naive Bayes\n",
    "print(\"Classification Report Naive Bayes:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aeeb50-f5d6-4c31-b0a2-0d5024be7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report SVC\n",
    "print(\"Classification Report SVC:\")\n",
    "print(classification_report(y_test, SvcPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fc4cb-915e-49c8-bbdf-985edac8d87d",
   "metadata": {},
   "source": [
    "<b>Naive Bayes is simple and fast, making it a good baseline for text classification tasks. SVM often performs better for complex data like text due to its ability to handle high-dimensional feature spaces and margins</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680612e-b088-4220-8462-4ec465a0dfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
